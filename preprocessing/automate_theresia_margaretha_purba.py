# -*- coding: utf-8 -*-
"""automate_Theresia Margaretha Purba.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gswgTHuhWRWxRqtrF4_MEyijNijEwwzM
"""

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

def preprocess_titanic(
    input_csv: str,
    output_dir: str = "titanic_preprocessing",
    target_col: str = "survived",
    test_size: float = 0.2,
    random_state: int = 42
):
    # Load data
    from google.colab import files
    files.upload()
    df = pd.read_csv(input_csv)
    df = df.copy()

    # =========================
    # 1) Tangani Missing Values
    # =========================
    if "age" in df.columns:
        df["age"] = df["age"].fillna(df["age"].median())

    if "fare" in df.columns:
        df["fare"] = df["fare"].fillna(df["fare"].median())

    if "embarked" in df.columns:
        df["embarked"] = df["embarked"].fillna(df["embarked"].mode()[0])

    # =========================
    # 2) Drop kolom tidak relevan / missing parah
    # =========================
    df = df.drop(columns=["cabin", "boat", "body", "home.dest", "name", "ticket"], errors="ignore")

    # =========================
    # 3) Hapus duplikat
    # =========================
    df = df.drop_duplicates()

    # =========================
    # 4) Split fitur & target
    # =========================
    if target_col not in df.columns:
        raise ValueError(f"Target column '{target_col}' tidak ditemukan. Kolom tersedia: {df.columns.tolist()}")

    X = df.drop(columns=[target_col])
    y = df[target_col]

    # =========================
    # 5) Split train/test (penting sebelum scaling!)
    # =========================
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y if y.nunique() <= 20 else None
    )

    # =========================
    # 6) Encoding kategorikal (fit di train, transform di test)
    # =========================
    cat_cols = [c for c in X_train.columns if X_train[c].dtype == "object"]
    num_cols = [c for c in X_train.columns if X_train[c].dtype != "object"]

    encoders = {}
    for col in cat_cols:
        le = LabelEncoder()
        X_train[col] = le.fit_transform(X_train[col].astype(str))
        X_test[col] = le.transform(X_test[col].astype(str))
        encoders[col] = le

    # =========================
    # 7) Standarisasi numerik (fit di train, transform di test)
    # =========================
    scaler = StandardScaler()
    if len(num_cols) > 0:
        X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
        X_test[num_cols] = scaler.transform(X_test[num_cols])

    # =========================
    # 8) Simpan output
    # =========================
    os.makedirs(output_dir, exist_ok=True)

    train_out = X_train.copy()
    train_out[target_col] = y_train.values
    train_path = os.path.join(output_dir, "train_preprocessed.csv")
    train_out.to_csv(train_path, index=False)

    test_out = X_test.copy()
    test_out[target_col] = y_test.values
    test_path = os.path.join(output_dir, "test_preprocessed.csv")
    test_out.to_csv(test_path, index=False)

    print("âœ… Preprocessing selesai.")
    print("Train:", train_out.shape, "| Test:", test_out.shape)
    print("Saved:", train_path, "and", test_path)

if __name__ == "__main__":
    preprocess_titanic(input_csv="../Titanic_raw/Titanic.csv",
                       output_dir="titanic_preprocessing")
